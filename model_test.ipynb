{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.5 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Demonstration of the CNN used for time series classification\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "In order to feed data to our model, we need to turn it into a keras sequence. This way it is fed to the network as batches of 1 sample each time, and there is no issue with the fact the time series have a different size. The whole preparation is done in the data_preparation function. The data are also shuffled."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/rwu-lh/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rwu-lh/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rwu-lh/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rwu-lh/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rwu-lh/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rwu-lh/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/rwu-lh/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rwu-lh/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rwu-lh/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rwu-lh/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rwu-lh/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rwu-lh/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Failed to load Python extension for LZ4 support. LZ4 compression will not be available.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/49/auto_sending_goals_2021-06-25-09-52-13 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/28/auto_sending_goals_2021-06-24-15-44-39 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/11/auto_sending_goals_2021-06-24-15-08-23 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/43/auto_sending_goals_2021-06-25-09-38-31 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/51/auto_sending_goals_2021-06-25-09-56-48 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/29/auto_sending_goals_2021-06-24-15-45-09 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/59/auto_sending_goals_2021-06-25-10-15-22 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/18/auto_sending_goals_2021-06-24-15-24-27 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/41/auto_sending_goals_2021-06-25-09-37-26 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/40/auto_sending_goals_2021-06-24-16-06-37 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/54/auto_sending_goals_2021-06-25-10-03-41 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/46/auto_sending_goals_2021-06-25-09-45-21 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/44/auto_sending_goals_2021-06-25-09-40-47 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/57/auto_sending_goals_2021-06-25-10-10-33 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/20/auto_sending_goals_2021-06-24-15-29-07 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/27/auto_sending_goals_2021-06-24-15-42-20 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/24/auto_sending_goals_2021-06-24-15-35-28 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/52/auto_sending_goals_2021-06-25-09-59-05 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/42/auto_sending_goals_2021-06-25-09-38-07 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/34/auto_sending_goals_2021-06-24-15-52-54 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/17/auto_sending_goals_2021-06-24-15-22-08 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/9/auto_sending_goals_2021-06-24-15-07-34 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/35/auto_sending_goals_2021-06-24-15-55-11 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/26/auto_sending_goals_2021-06-24-15-40-03 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/21/auto_sending_goals_2021-06-24-15-32-06 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/50/auto_sending_goals_2021-06-25-09-54-30 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/23/auto_sending_goals_2021-06-24-15-33-11 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/2/auto_sending_goals_2021-06-24-15-00-41 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/6/auto_sending_goals_2021-06-24-15-04-15 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/60/auto_sending_goals_2021-06-25-10-17-46 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/3/auto_sending_goals_2021-06-24-15-01-05 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/15/auto_sending_goals_2021-06-24-15-17-31 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/37/auto_sending_goals_2021-06-24-15-59-46 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/19/auto_sending_goals_2021-06-24-15-26-47 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/8/auto_sending_goals_2021-06-24-15-07-09 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/7/auto_sending_goals_2021-06-24-15-04-52 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/5/auto_sending_goals_2021-06-24-15-01-58 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/14/auto_sending_goals_2021-06-24-15-15-14 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/53/auto_sending_goals_2021-06-25-10-01-24 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/16/auto_sending_goals_2021-06-24-15-19-48 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/56/auto_sending_goals_2021-06-25-10-08-16 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/1/auto_sending_goals_2021-06-24-15-00-02 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/55/auto_sending_goals_2021-06-25-10-05-58 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/36/auto_sending_goals_2021-06-24-15-57-28 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/47/auto_sending_goals_2021-06-25-09-47-39 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/12/auto_sending_goals_2021-06-24-15-10-40 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/33/auto_sending_goals_2021-06-24-15-50-37 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/25/auto_sending_goals_2021-06-24-15-37-45 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/32/auto_sending_goals_2021-06-24-15-48-19 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/45/auto_sending_goals_2021-06-25-09-43-04 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/13/auto_sending_goals_2021-06-24-15-12-57 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/31/auto_sending_goals_2021-06-24-15-46-02 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/38/auto_sending_goals_2021-06-24-16-02-03 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/22/auto_sending_goals_2021-06-24-15-32-45 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/58/auto_sending_goals_2021-06-25-10-12-54 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/4/auto_sending_goals_2021-06-24-15-01-30 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/48/auto_sending_goals_2021-06-25-09-49-56 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/39/auto_sending_goals_2021-06-24-16-04-20 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/30/auto_sending_goals_2021-06-24-15-45-33 already exists. Not creating.\n",
      "[INFO]  Data folder ../20_scan_odom_noise/10/auto_sending_goals_2021-06-24-15-07-59 already exists. Not creating.\n"
     ]
    }
   ],
   "source": [
    "from cnn import Turtlebot_CNN,data_preparation\n",
    "from read_data import make_dataset,read_bag\n",
    "\n",
    "train,test=data_preparation(path='../20_scan_odom_noise/',training=0.7)#The training parameters determines how much of the dataset is dedicated to training (1=100%)\n"
   ]
  },
  {
   "source": [
    "## Training the Model\n",
    "\n",
    "We can now train the model using the fit method."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From /home/rwu-lh/.local/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Epoch 1/1000000\n",
      "WARNING:tensorflow:Entity <bound method Turtlebot_CNN.call of <cnn.Turtlebot_CNN object at 0x7f22931c2450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Turtlebot_CNN.call of <cnn.Turtlebot_CNN object at 0x7f22931c2450>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Turtlebot_CNN.call of <cnn.Turtlebot_CNN object at 0x7f22931c2450>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Turtlebot_CNN.call of <cnn.Turtlebot_CNN object at 0x7f22931c2450>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "Tensor(\"turtlebot_cnn/dense/Relu:0\", shape=(?, 64), dtype=float32)\n",
      "WARNING:tensorflow:From /home/rwu-lh/.local/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "42/42 [==============================] - 98s 2s/step - loss: nan\n",
      "Epoch 2/1000000\n",
      " 2/42 [>.............................] - ETA: 2:41 - loss: nan"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2885f850fe5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m           initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    674\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager_dataset_or_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m       \u001b[0;31m# Make sure that y, sample_weights, validation_split are not passed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model=Turtlebot_CNN(type=\"lstm\")\n",
    "model.compile(optimizer=opt,loss='categorical_crossentropy')\n",
    "model.fit(train,epochs=1000000,callbacks=[tf.keras.callbacks.EarlyStopping(monitor='loss',patience=10)])\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "## Testing the model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Here is a Visualization of what the CNN does \n",
    "\n",
    "![Visualization of the CNN](img/Schema_CNN.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def results(model,test):\n",
    "    res=model.predict(test)\n",
    "    c=0\n",
    "    for i,pred in enumerate(res):\n",
    "        y=test[i][1][0]\n",
    "        if np.argmax(y)==np.argmax(pred):\n",
    "            c+=1\n",
    "    return(c/len(res))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[nan nan nan]\n [nan nan nan]\n [nan nan nan]\n [nan nan nan]\n [nan nan nan]\n [nan nan nan]\n [nan nan nan]\n [nan nan nan]\n [nan nan nan]\n [nan nan nan]\n [nan nan nan]\n [nan nan nan]\n [nan nan nan]\n [nan nan nan]\n [nan nan nan]\n [nan nan nan]\n [nan nan nan]\n [nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[4.61116906e-05, 4.68472484e-03, 9.95269120e-01],\n",
       "       [9.25233364e-01, 7.21699595e-02, 2.59670289e-03],\n",
       "       [3.78366676e-03, 9.94905591e-01, 1.31075468e-03],\n",
       "       [2.62500113e-03, 9.96960223e-01, 4.14754410e-04],\n",
       "       [9.99458134e-01, 5.41834394e-04, 7.68437258e-10],\n",
       "       [4.59864968e-05, 4.84398659e-03, 9.95109975e-01],\n",
       "       [5.07719451e-05, 5.45975054e-03, 9.94489431e-01],\n",
       "       [9.31484044e-01, 6.65853396e-02, 1.93071004e-03],\n",
       "       [4.57072347e-05, 4.71062446e-03, 9.95243609e-01],\n",
       "       [9.99448717e-01, 5.51279052e-04, 7.80687348e-10],\n",
       "       [4.12140302e-02, 9.41440642e-01, 1.73454005e-02],\n",
       "       [7.48845850e-05, 7.32398406e-03, 9.92601097e-01],\n",
       "       [3.08108311e-02, 9.69189227e-01, 3.68011444e-09],\n",
       "       [9.87455904e-01, 1.25435311e-02, 6.13843611e-07],\n",
       "       [5.80834676e-05, 5.69286477e-03, 9.94248986e-01],\n",
       "       [1.03753105e-01, 4.53221023e-01, 4.43025947e-01],\n",
       "       [9.99992013e-01, 7.92802894e-06, 2.69331119e-16],\n",
       "       [3.78128910e-03, 9.94909227e-01, 1.30943127e-03]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([array([[[-8.63533344e-01,  2.27637017e+00, -1.00239779e-03, ...,\n",
       "            0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "          [-8.63533354e-01,  2.27637018e+00, -1.00239778e-03, ...,\n",
       "            0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "          [-8.63533364e-01,  2.27637019e+00, -1.00239778e-03, ...,\n",
       "            0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "          ...,\n",
       "          [-8.63569450e-01,  2.27641616e+00, -1.00239944e-03, ...,\n",
       "            0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "          [-8.63569460e-01,  2.27641617e+00, -1.00239945e-03, ...,\n",
       "            0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "          [-8.63569469e-01,  2.27641618e+00, -1.00239944e-03, ...,\n",
       "            0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]]),\n",
       "  array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]]])],\n",
       " array([[0, 0, 1]]))"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}